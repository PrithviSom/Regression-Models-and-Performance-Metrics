{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "409e40c1",
   "metadata": {},
   "source": [
    "\n",
    "Question 1 : What is Simple Linear Regression (SLR)? Explain its purpose.\n",
    "\n",
    "ans : Simple Linear Regression (SLR) is a statistical method used to model the relationship between two continuous variables: one independent (predictor) and one dependent (outcome). Its purpose is to quantify this relationship with a straight line, allowing us to both understand how the variables are related and make predictions about the dependent variable based on the independent variable.\n",
    "\n",
    "Understanding Relationships\n",
    "- SLR helps determine whether and how strongly two variables are related.\n",
    "- Example: Studying how hours studied (X) affect exam scores (Y).\n",
    "\n",
    "Prediction\n",
    "- Once the line is fitted, you can predict the dependent variable for new values of the independent variable.\n",
    "- Example: Predicting a student’s exam score if they study for 10 hours.\n",
    "\n",
    "Quantifying Impact\n",
    "- The slope tells us the magnitude of change in Y for each unit increase in X.\n",
    "- Example: If slope = 5, each extra hour of study increases the score by 5 points.\n",
    "\n",
    "Decision-Making\n",
    "- Used in economics, business, engineering, and science to guide decisions based on data-driven predictions.\n",
    "\n",
    "\n",
    "Question 2: What are the key assumptions of Simple Linear Regression?\n",
    "\n",
    "ans:  \n",
    " Linearity\n",
    "- The relationship between the independent variable X and dependent variable Y is linear.\n",
    "\n",
    "Independence of Errors\n",
    "- Residuals (errors) are independent; no autocorrelation exists.\n",
    "\n",
    "Homoscedasticity\n",
    "- The variance of residuals is constant across all values of X.\n",
    "\n",
    "Normality of Errors\n",
    "- Residuals should be approximately normally distributed.\n",
    "\n",
    "No Perfect Multicollinearity\n",
    "- In SLR, only one predictor is used, so this is trivially satisfied. (Relevant in multiple regression.)\n",
    "\n",
    "Measurement Accuracy of Predictor\n",
    "- The independent variable X is measured without significant error.\n",
    "\n",
    "Fixed Independent Variable\n",
    "- Values of X are considered fixed in repeated samples (not random).\n",
    "\n",
    "Additivity of Effects\n",
    "- The effect of X on Y is additive, captured by slope + intercept.\n",
    "\n",
    " Correct Model Specification\n",
    "- The model includes the right variables and form; no important predictors are omitted.\n",
    "\n",
    " Error Term Expectation = 0\n",
    "- The average of residuals is zero, meaning the regression line is unbiased.\n",
    "\n",
    "\n",
    "Question 3: Write the mathematical equation for a simple linear regression model and\n",
    "explain each term.\n",
    "\n",
    "ans : \n",
    "Explanation of Each Term\n",
    "\n",
    "Y (Dependent Variable / Response Variable)\n",
    "- The outcome we are trying to predict or explain.\n",
    "- Example: Exam score, house price, sales revenue.\n",
    "\n",
    " X (Independent Variable / Predictor Variable)\n",
    "- The input variable used to predict Y.\n",
    "- Example: Hours studied, square footage, advertising spend.\n",
    "\n",
    " (Intercept)\n",
    "- The value of Y when X=0.\n",
    "- It represents the baseline level of the dependent variable.\n",
    "- Example: If no hours are studied, the expected exam score might still be 20 due to prior knowledge.\n",
    "\n",
    " (Slope Coefficient)\n",
    "- The change in Y for a one-unit increase in X.\n",
    "- It quantifies the strength and direction of the relationship.\n",
    "- Example: If Slope Coefficient=5, each extra hour of study increases the exam score by 5 points.\n",
    "\n",
    "(Error Term / Residual)\n",
    "- Captures the variation in Y not explained by X.\n",
    "- Represents randomness, measurement error, or other unobserved factors.\n",
    "- Example: Two students studying the same hours may still score differently due to motivation, health, or luck.\n",
    "\n",
    "Question 4: Provide a real-world example where simple linear regression can be\n",
    "applied.\n",
    "\n",
    "ans : \n",
    " Real-World Example of Simple Linear Regression\n",
    "Scenario: Predicting house prices based on square footage.\n",
    "\n",
    "1. Dependent Variable (Y)\n",
    "- House price (in ₹ or $).\n",
    "- This is what we want to predict.\n",
    "2. Independent Variable (X)\n",
    "- Square footage (size of the house).\n",
    "- This is the predictor.\n",
    "3. Regression Equation\n",
    " $y = \\beta_{0} + \\beta_{1}x + \\epsilon$\n",
    "- $\\beta_{0}$: Base price (intercept) when square footage is 0.\n",
    "- $\\beta_{1}$: Increase in price per additional square foot.\n",
    "- $\\epsilon$ : Error term (captures other factors like location, age, amenities).\n",
    "\n",
    "\n",
    "Question 5: What is the method of least squares in linear regression?\n",
    "\n",
    "ans : \n",
    "\n",
    "The method of least squares is the standard technique used to estimate the parameters \n",
    "(\\(\\beta_0, \\beta_1\\)) of a linear regression model.  \n",
    "It works by finding the line that minimizes the sum of the squared differences between \n",
    "the observed values and the values predicted by the regression line.\n",
    "\n",
    "For a regression model:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 x + \\epsilon$\n",
    "\n",
    "We want to choose ($\\beta_0$) and ($\\beta_1$) such that the residuals (errors) are as small as possible.\n",
    "\n",
    "- Residual for each data point:\n",
    "\n",
    "$e_i = y_i - (\\beta_0 + \\beta_1 x_i)$\n",
    "\n",
    "\n",
    "- Least squares minimizes:\n",
    "\n",
    "$S = \\sum_{i=1}^{n} \\big(y_i - (\\beta_0 + \\beta_1 x_i)\\big)^2$\n",
    "\n",
    "\n",
    "This ensures the regression line is the best fit line through the data.\n",
    "\n",
    " Purpose\n",
    "- Find the best-fit line that represents the relationship between (x) and (y).  \n",
    "- Minimize error by reducing the squared differences between actual and predicted values.  \n",
    "- Provide estimates of slope ($\\beta_1$)  and intercept ($\\beta_0$)  that can be used for prediction.  \n",
    "\n",
    "\n",
    "Example\n",
    "Suppose we want to predict exam scores (y) from hours studied (x).\n",
    "\n",
    "- Actual data points:  \n",
    "  - (2 hrs → 50 marks)  \n",
    "  - (4 hrs → 70 marks)  \n",
    "  - (6 hrs → 90 marks)  \n",
    "\n",
    "Least squares finds the line that minimizes the squared differences between actual scores and predicted scores.\n",
    "\n",
    "\n",
    "Question 6: What is Logistic Regression? How does it differ from Linear Regression?\n",
    "\n",
    "ans: \n",
    "\n",
    "Nature of Output\n",
    "- Linear Regression predicts continuous values.\n",
    "- Logistic Regression predicts probabilities (between 0 and 1).\n",
    "\n",
    "Dependent Variable Type\n",
    "- Linear Regression → Continuous (e.g., salary, house price).\n",
    "- Logistic Regression → Categorical (e.g., pass/fail, spam/not spam).\n",
    "\n",
    " Curve vs Line\n",
    "- Linear Regression fits a straight line.\n",
    "- Logistic Regression fits an S‑shaped curve (sigmoid).\n",
    "\n",
    "Interpretation of Coefficients\n",
    "- Linear Regression: $\\beta _1$ = change in y per unit change in x.\n",
    "- Logistic Regression: $\\beta _1$ = change in log‑odds of the outcome per unit change in x.\n",
    "\n",
    "Error Minimization\n",
    "- Linear Regression minimizes sum of squared errors.\n",
    "- Logistic Regression maximizes likelihood (log‑likelihood).\n",
    "\n",
    "Range of Predictions\n",
    "- Linear Regression predictions can be any real number.\n",
    "- Logistic Regression predictions are constrained to [0,1].\n",
    "\n",
    "Use Cases\n",
    "- Linear Regression → predicting quantities (e.g., sales, temperature).\n",
    "- Logistic Regression → classification (e.g., disease/no disease, churn/no churn).\n",
    "\n",
    "Assumptions\n",
    "- Linear Regression assumes linearity, normality of errors, homoscedasticity.\n",
    "- Logistic Regression assumes linearity in the log‑odds, not in the raw outcome.\n",
    "\n",
    "Decision Making\n",
    "- Linear Regression → directly uses predicted values.\n",
    "- Logistic Regression → applies a threshold (e.g., 0.5) to classify into categories.\n",
    "\n",
    "\n",
    "Question 7: Name and briefly describe three common evaluation metrics for regression\n",
    "models.\n",
    "\n",
    "ans : \n",
    "\n",
    "Mean Absolute Error (MAE)\n",
    "- Measures the average of the absolute differences between predicted and actual values.\n",
    "- Easy to interpret: “On average, predictions are off by X units.”\n",
    "- Treats all errors equally, without squaring.\n",
    "\n",
    " Mean Squared Error (MSE)\n",
    "- Calculates the average of squared differences between predicted and actual values.\n",
    "- Penalizes larger errors more heavily because of squaring.\n",
    "- Useful when big mistakes are particularly undesirable.\n",
    "\n",
    " R-squared (R^2) — Coefficient of Determination\n",
    "- Represents the proportion of variance in the dependent variable explained by the model.\n",
    "- Ranges from 0 to 1, with values closer to 1 indicating a better fit.\n",
    "- Shows how well the regression line captures the data’s variability.\n",
    "\n",
    "\n",
    "Question 8: What is the purpose of the R-squared metric in regression analysis?\n",
    "\n",
    "ans: The R-squared (R^2) metric, also known as the coefficient of determination, serves the purpose of measuring how well a regression model explains the variability of the dependent variable. It represents the proportion of variance in the outcome that is accounted for by the independent variables, essentially indicating the goodness of fit of the model. A higher R^2 value, closer to 1, means the model explains a larger share of the variation in the data, while a lower value suggests the model captures less of the variability. In regression analysis, R^2 helps assess the explanatory power of the model and provides insight into how effectively the predictors describe the relationship with the target variable.\n",
    "\n",
    "\n",
    "Question 10: How do you interpret the coefficients in a simple linear regression model?\n",
    "\n",
    "ans:\n",
    " A simple linear regression model is written as:\n",
    "\n",
    "$y=\\beta _0+\\beta _1x+\\epsilon$ \n",
    "\n",
    "Where:\n",
    "- y = dependent variable (outcome)\n",
    "- x = independent variable (predictor)\n",
    "- $\\beta _0$ = intercept\n",
    "- $\\beta _1$ = slope (coefficient of x)\n",
    "- $\\epsilon$  = error term\n",
    "\n",
    "Interpretation\n",
    " Intercept ($\\beta _0$)\n",
    "- Represents the expected value of y when x=0.\n",
    "- It’s the baseline or starting point of the regression line.\n",
    "- Example: If $\\beta _0$=50, then when study hours = 0, the predicted exam score is 50.\n",
    "\n",
    "Slope ($\\beta _1$)\n",
    "- Represents the change in y for a one-unit increase in x.\n",
    "- Shows the strength and direction of the relationship between x and y.\n",
    "- Example: If $\\beta _1$=10, then each additional study hour increases the predicted exam score by 10 points.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ec53b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eda02dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\prith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.24.1 in c:\\users\\prith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\prith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\prith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\prith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5be2322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope (β1): 0.6\n",
      "Intercept (β0): 2.2\n"
     ]
    }
   ],
   "source": [
    "# Question 9: Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept.\n",
    "\n",
    "# ans :\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data (X = independent variable, y = dependent variable)\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)   # predictor values\n",
    "y = np.array([2, 4, 5, 4, 5])                  # target values\n",
    "\n",
    "# Create and fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Print slope (coefficient) and intercept\n",
    "print(\"Slope (β1):\", model.coef_[0])\n",
    "print(\"Intercept (β0):\", model.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6f7518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
